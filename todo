RESPONSES/DB
- export/working responses
- working block names, in responses
- form at beginning/end of study, data stored (not as response) -- form specified in config file
  - AND THEN USE THIS for responses! e.g. collecting the particpants' age, etc.
  should be easy to insert a form before or afterward, and store the data properly (different table than responses)

- yaml
- scaling
- might need to bring back deepshuffle - to hit all blocks and they don't need to say it each time
  - might be an antipattern though -- or at least, may require explicit "shuffle: none" on lower levels that shouldn't be
- sequences

- validation (more) and error handling
- tests
- stricter typing, docblocks, docs

- let them just give us css? fully formed web components and just run them? interface to store responses (or overlay via config file)


THINGS TO MAKE MY LIFE EASIER:
- separate assets/runlist for testing
- tests set up on services


  for sequence:
    - just create additional object, like 'usedResponses' and pass it inside or alongside params
      - generator doesn't need to know anything about any of it
  - responses now passed along as list to generator (though it may end up storing them itself in the future)

- working config/runner
    - params give us response object
    - at the end of it, could just have a marker of 'responder'
- get conditional choices working through generator and back

RESPONDERS:
  pulled into the stimuli components if they have anything that is a StimuliResponse type in their parameters (scan? label?)
    else, just fire the doneEvent() when finished, after a parameter-set delay, etc - or the action/frame itself can receive the click..
      or it just defaults to a conditional of layering its entire self with a clickable div that fires done()

FRAME:
  its own component, which handles multiple responses/done events from multiple stimuli, and then passes this up to the parent that handles
    the frame/action itself, and iterates to the next one
  parameters to the frame would be the placement of the stimuli, or it would get this from the stimuli config itself - think on it


flow is as events --
  stimuli -> appComponent/response aggregator/Frame -> <- service (in reverse)
response sent stimuli->parent, it decides what to let through based on Control settings/param/whatever the child tells it

TODO - move all of this into specific sections rather than global


DEPLOY:
  - query/store GKE/EKS account credentials
  - direct api to GKE/EKS to spin up nodes -- and create buckets - no TF?
    - make new resource, store reference to resource
  - direct api to send over kube pods
  - pull down key for auth
    - look into how packer does it. without that key I can't get into the instance flat-out, as ssh is the only way in/out
    - here it would be an api, can use the same key for both?
    - look into how it's done on kube
    - make sure it's all in gitignore/stored outside the directory
    - could also put a passphrase on it just in case

    - could also store it in bucket accessible by aws/eks lab member accounts
    - or just that lab account, and administrator account

    - if credentials are never stored, but rather requested every time and instance (api) data accessible via aws/eks/oauth account..
      - that could be better. much better
      - the key used for creation could have lab-known passphrase and be on bucket accessible to just their accounts
      - panda and this login should be oauth/sso anyway. even if this is just at shell interface stage
PULL:
  - read reference to resource
  - connect over some api call rather than ssh, use key to get to it
  - .. figure it out
  - could also just store it in bucket that lab members can access
DESTROY:
  - first PULL all data down to local archive
  - reverse of DEPLOY

LOG -- log the ENTIRE sequence, essentially log the Frames that were traversed, the final tree that the participant experienced,
so it is very clear what occurred
  - could literally print tree mapped to responses (would be nifty feature) in addition to logging the Frame they were in, on
    the Response/eventual csv row
  - could make super nice svg render of it (and text - strategy pattern (w/-mac.txt and -pc.txt for newline variance))
    - or just switch that asks for svg or txt.
    - or just output all into the final zip - csv, svg, txt files
  - and that's what is output from export button in the app, too
  - so just the csv, txt and svg renderers on Response service export()



later:
  - this respository should probably be treated as a dependency, and their projects should be nothing other than:
    package.json
    runsheet.yaml
    assets/

    - and then npm run
      - init, test/serve/develop, deploy, pull, destroy, etc.
      - deploy/pull/destroy just forward to the actual toku package.json call, none of that is in this scaffold repo
      - so it checks and runs init etc. first before actually calling those internally


